---
title: LLM 采样
sidebarTitle: 采样
description: 处理服务器发起的 LLM 采样请求。
icon: robot
---

import { VersionBadge } from "/snippets/version-badge.mdx";

<VersionBadge version="2.0.0" />

MCP 服务器可以向客户端请求 LLM 完成。客户端通过采样处理程序回调来处理这些请求。

## 采样处理程序

在创建客户端时提供 `sampling_handler` 函数：

```python
from fastmcp import Client
from fastmcp.client.sampling import (
    SamplingMessage,
    SamplingParams,
    RequestContext,
)

async def sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext
) -> str:
    # 您的 LLM 集成逻辑在此
    # 从消息中提取文本并生成响应
    return "基于消息生成的响应"

client = Client(
    "my_mcp_server.py",
    sampling_handler=sampling_handler,
)
```

### 处理程序参数

采样处理程序接收三个参数：

<Card icon="code" title="采样处理程序参数">
<ResponseField name="SamplingMessage" type="Sampling Message Object">
  <Expandable title="attributes">
    <ResponseField name="role" type='Literal["user", "assistant"]'>
      消息的角色。
    </ResponseField>

    <ResponseField name="content" type="TextContent | ImageContent | AudioContent">
      消息的内容。

      TextContent 最常见，具有 `.text` 属性。
    </ResponseField>

  </Expandable>
</ResponseField>
<ResponseField name="SamplingParams" type="Sampling Parameters Object">
  <Expandable title="attributes">
    <ResponseField name="messages" type="list[SamplingMessage]">
      要采样的消息
    </ResponseField>

    <ResponseField name="modelPreferences" type="ModelPreferences | None">
      服务器对选择哪个模型的偏好。客户端可以忽略
    这些偏好。
    <Expandable title="attributes">
      <ResponseField name="hints" type="list[ModelHint] | None">
        用于模型选择的提示。
      </ResponseField>

      <ResponseField name="costPriority" type="float | None">
        模型选择的成本优先级。
      </ResponseField>

      <ResponseField name="speedPriority" type="float | None">
        模型选择的速度优先级。
      </ResponseField>

      <ResponseField name="intelligencePriority" type="float | None">
        模型选择的智能优先级。
      </ResponseField>
    </Expandable>
    </ResponseField>

    <ResponseField name="systemPrompt" type="str | None">
      服务器希望用于采样的可选系统提示。
    </ResponseField>

    <ResponseField name="includeContext" type="IncludeContext | None">
      请求包含来自一个或多个 MCP 服务器（包括调用者）的上下文，
      附加到提示中。
    </ResponseField>

    <ResponseField name="temperature" type="float | None">
      采样温度。
    </ResponseField>

    <ResponseField name="maxTokens" type="int">
      要采样的最大令牌数。
    </ResponseField>

    <ResponseField name="stopSequences" type="list[str] | None">
      用于采样的停止序列。
    </ResponseField>

    <ResponseField name="metadata" type="dict[str, Any] | None">
      传递给 LLM 提供者的可选元数据。
    </ResponseField>
    </Expandable>

</ResponseField>
<ResponseField name="RequestContext" type="Request Context Object">
  <Expandable title="attributes">
    <ResponseField name="request_id" type="RequestId">
      MCP 请求的唯一标识符
    </ResponseField>
  </Expandable>
</ResponseField>
</Card>

## 基本示例

```python
from fastmcp import Client
from fastmcp.client.sampling import SamplingMessage, SamplingParams, RequestContext

async def basic_sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext
) -> str:
    # 提取消息内容
    conversation = []
    for message in messages:
        content = message.content.text if hasattr(message.content, 'text') else str(message.content)
        conversation.append(f"{message.role}: {content}")

    # 如果提供了系统提示则使用它
    system_prompt = params.systemPrompt or "您是一个乐于助人的助手。"

    # 在这里您将与您首选的 LLM 服务集成
    # 这只是一个占位符响应
    return f"基于对话的响应: {' | '.join(conversation)}"

client = Client(
    "my_mcp_server.py",
    sampling_handler=basic_sampling_handler
)
```

<Note>
如果客户端未提供采样处理器，服务器可以选择配置回退处理器。详情请参阅[服务器采样](/zh/servers/sampling#sampling-fallback-handler)。
</Note>
