---
title: LLM 采样
sidebarTitle: 采样
description: 处理服务器发起的 LLM 采样请求。
icon: robot
---

import { VersionBadge } from "/snippets/version-badge.mdx";

<VersionBadge version="2.0.0" />

MCP 服务器可以向客户端请求 LLM 完成。客户端通过采样处理程序回调来处理这些请求。

## 采样处理程序

在创建客户端时提供 `sampling_handler` 函数：

```python
from fastmcp import Client
from fastmcp.client.sampling import (
    SamplingMessage,
    SamplingParams,
    RequestContext,
)

async def sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext
) -> str:
    # 您的 LLM 集成逻辑在此
    # 从消息中提取文本并生成响应
    return "基于消息生成的响应"

client = Client(
    "my_mcp_server.py",
    sampling_handler=sampling_handler,
)
```

### 处理程序参数

采样处理程序接收三个参数：

<Card icon="code" title="采样处理程序参数">
<ResponseField name="SamplingMessage" type="Sampling Message Object">
  <Expandable title="attributes">
    <ResponseField name="role" type='Literal["user", "assistant"]'>
      消息的角色。
    </ResponseField>

    <ResponseField name="content" type="TextContent | ImageContent | AudioContent">
      消息的内容。

      TextContent 最常见，具有 `.text` 属性。
    </ResponseField>

  </Expandable>
</ResponseField>
<ResponseField name="SamplingParams" type="Sampling Parameters Object">
  <Expandable title="attributes">
    <ResponseField name="messages" type="list[SamplingMessage]">
      要采样的消息
    </ResponseField>

    <ResponseField name="modelPreferences" type="ModelPreferences | None">
      服务器对选择哪个模型的偏好。客户端可以忽略
    这些偏好。
    <Expandable title="attributes">
      <ResponseField name="hints" type="list[ModelHint] | None">
        用于模型选择的提示。
      </ResponseField>

      <ResponseField name="costPriority" type="float | None">
        模型选择的成本优先级。
      </ResponseField>

      <ResponseField name="speedPriority" type="float | None">
        模型选择的速度优先级。
      </ResponseField>

      <ResponseField name="intelligencePriority" type="float | None">
        模型选择的智能优先级。
      </ResponseField>
    </Expandable>
    </ResponseField>

    <ResponseField name="systemPrompt" type="str | None">
      服务器希望用于采样的可选系统提示。
    </ResponseField>

    <ResponseField name="includeContext" type="IncludeContext | None">
      请求包含来自一个或多个 MCP 服务器（包括调用者）的上下文，
      附加到提示中。
    </ResponseField>

    <ResponseField name="temperature" type="float | None">
      采样温度。
    </ResponseField>

    <ResponseField name="maxTokens" type="int">
      要采样的最大令牌数。
    </ResponseField>

    <ResponseField name="stopSequences" type="list[str] | None">
      用于采样的停止序列。
    </ResponseField>

    <ResponseField name="metadata" type="dict[str, Any] | None">
      传递给 LLM 提供者的可选元数据。
    </ResponseField>
    </Expandable>

</ResponseField>
<ResponseField name="RequestContext" type="Request Context Object">
  <Expandable title="attributes">
    <ResponseField name="request_id" type="RequestId">
      MCP 请求的唯一标识符
    </ResponseField>
  </Expandable>
</ResponseField>
</Card>

## 基本示例

```python
from fastmcp import Client
from fastmcp.client.sampling import SamplingMessage, SamplingParams, RequestContext

async def basic_sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext
) -> str:
    # 提取消息内容
    conversation = []
    for message in messages:
        content = message.content.text if hasattr(message.content, 'text') else str(message.content)
        conversation.append(f"{message.role}: {content}")

    # 如果提供了系统提示则使用它
    system_prompt = params.systemPrompt or "您是一个乐于助人的助手。"

    # 在这里您将与您首选的 LLM 服务集成
    # 这只是一个占位符响应
    return f"基于对话的响应: {' | '.join(conversation)}"

client = Client(
    "my_mcp_server.py",
    sampling_handler=basic_sampling_handler
)
```

## 采样回退


客户端对采样的支持是可选的，如果客户端不支持采样，服务器将报告错误，指示客户端不支持采样。


也可以为 FastMCP 服务器提供 `sampling_handler`，当客户端不支持采样时，它将用于处理采样请求。此采样处理器绕过客户端，直接向 LLM 提供商发送采样请求。

采样处理器可以使用任何 LLM 提供商来实现，但作为贡献模块提供了 OpenAI 的示例实现。采样缺乏典型 LLM 补全的完整功能。因此，指向第三方提供商的 OpenAI 兼容 API 的 OpenAI 采样处理器通常足以实现采样处理器。



```python
import asyncio
import os

from mcp.types import ContentBlock
from openai import OpenAI

from fastmcp import FastMCP
from fastmcp.experimental.sampling.handlers.openai import OpenAISamplingHandler
from fastmcp.server.context import Context


async def async_main():
    server = FastMCP(
        name="OpenAI Sampling Fallback Example",
        sampling_handler=OpenAISamplingHandler(
            default_model="gpt-4o-mini",
            client=OpenAI(
                api_key=os.getenv("API_KEY"),
                base_url=os.getenv("BASE_URL"),
            ),
        ),
    )

    @server.tool
    async def test_sample_fallback(ctx: Context) -> ContentBlock:
        return await ctx.sample(
            messages=["hello world!"],
        )

    await server.run_http_async()


if __name__ == "__main__":
    asyncio.run(async_main())
```