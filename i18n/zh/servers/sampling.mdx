---
title: LLM 采样
sidebarTitle: 采样
description: 通过 MCP 上下文请求客户端的 LLM 基于提供的消息生成文本。
icon: robot
---

import { VersionBadge } from '/snippets/version-badge.mdx'

<VersionBadge version="2.0.0" />

LLM 采样允许 MCP 工具请求客户端的 LLM 基于提供的消息生成文本。当工具需要利用 LLM 的能力来处理数据、生成响应或执行基于文本的分析时，这非常有用。

## 为什么使用 LLM 采样？

LLM 采样使工具能够：

- **利用 AI 能力**：使用客户端的 LLM 进行文本生成和分析
- **减轻复杂推理负担**：让 LLM 处理需要自然语言理解的任务
- **生成动态内容**：基于数据创建响应、摘要或转换
- **保持上下文**：使用用户已在交互的同一 LLM 实例

### 基本用法

使用 `ctx.sample()` 请求客户端的 LLM 生成文本：

```python {14}
from fastmcp import FastMCP, Context

mcp = FastMCP("SamplingDemo")

@mcp.tool
async def analyze_sentiment(text: str, ctx: Context) -> dict:
    """使用客户端的 LLM 分析文本情感。"""
    prompt = f"""分析以下文本的情感为积极、消极或中性。
    只输出一个词 - '积极'、'消极' 或 '中性'。
    
    要分析的文本：{text}"""
    
    # 请求 LLM 分析
    response = await ctx.sample(prompt)
    
    # 处理 LLM 的响应
    sentiment = response.text.strip().lower()
    
    # 映射到标准情感值
    if "积极" in sentiment or "positive" in sentiment:
        sentiment = "积极"
    elif "消极" in sentiment or "negative" in sentiment:
        sentiment = "消极"
    else:
        sentiment = "中性"
    
    return {"text": text, "sentiment": sentiment}
```

## 方法签名

<Card icon="code" title="上下文采样方法">
<ResponseField name="ctx.sample" type="异步方法">
  从客户端的 LLM 请求文本生成
  
  <Expandable title="参数">
    <ResponseField name="messages" type="str | list[str | SamplingMessage]">
      发送给 LLM 的字符串或字符串/消息对象列表
    </ResponseField>
    
    <ResponseField name="system_prompt" type="str | None" default="None">
      用于指导 LLM 行为的可选系统提示
    </ResponseField>
    
    <ResponseField name="temperature" type="float | None" default="None">
      可选的采样温度（控制随机性，通常为 0.0-1.0）
    </ResponseField>
    
    <ResponseField name="max_tokens" type="int | None" default="512">
      可选的最大生成令牌数
    </ResponseField>
    
    <ResponseField name="model_preferences" type="ModelPreferences | str | list[str] | None" default="None">
      可选的模型选择偏好（例如，模型提示字符串、提示列表或 ModelPreferences 对象）
    </ResponseField>
  </Expandable>
  
  <Expandable title="响应">
    <ResponseField name="response" type="TextContent | ImageContent">
      LLM 的响应内容（通常是带有 .text 属性的 TextContent）
    </ResponseField>
  </Expandable>
</ResponseField>
</Card>

## 简单文本生成

### 基本提示

使用简单字符串提示生成文本：

```python {6}
@mcp.tool
async def generate_summary(content: str, ctx: Context) -> str:
    """生成提供内容的摘要。"""
    prompt = f"请提供以下内容的简洁摘要：\n\n{content}"
    
    response = await ctx.sample(prompt)
    return response.text
```

### 系统提示

使用系统提示来指导 LLM 的行为：

```python {4-9}
@mcp.tool
async def generate_code_example(concept: str, ctx: Context) -> str:
    """为给定概念生成 Python 代码示例。"""
    response = await ctx.sample(
        messages=f"编写一个演示 '{concept}' 的简单 Python 代码示例。",
        system_prompt="你是一个专业的 Python 程序员。提供简洁、可运行的代码示例，不需要解释。",
        temperature=0.7,
        max_tokens=300
    )
    
    code_example = response.text
    return f"```python\n{code_example}\n```"
```


### 模型偏好

为不同用例指定模型偏好：

```python {4-8, 17-22}
@mcp.tool
async def creative_writing(topic: str, ctx: Context) -> str:
    """使用特定模型生成创意内容。"""
    response = await ctx.sample(
        messages=f"写一个关于 {topic} 的创意短篇小说",
        model_preferences="claude-3-sonnet",  # 偏好特定模型
        include_context="thisServer",  # 使用服务器的上下文
        temperature=0.9,  # 高创造性
        max_tokens=1000
    )
    
    return response.text

@mcp.tool
async def technical_analysis(data: str, ctx: Context) -> str:
    """使用专注推理的模型执行技术分析。"""
    response = await ctx.sample(
        messages=f"分析这些技术数据并提供见解：{data}",
        model_preferences=["claude-3-opus", "gpt-4"],  # 偏好推理模型
        temperature=0.2,  # 低随机性保证一致性
        max_tokens=800
    )
    
    return response.text
```

### 复杂消息结构

使用结构化消息进行更复杂的交互：

```python {1, 6-10}
from fastmcp.client.sampling import SamplingMessage

@mcp.tool
async def multi_turn_analysis(user_query: str, context_data: str, ctx: Context) -> str:
    """使用多轮对话结构执行分析。"""
    messages = [
        SamplingMessage(role="user", content=f"我有这些数据：{context_data}"),
        SamplingMessage(role="assistant", content="我能看到您的数据。您希望我分析什么？"),
        SamplingMessage(role="user", content=user_query)
    ]
    
    response = await ctx.sample(
        messages=messages,
        system_prompt="你是一个数据分析师。基于对话上下文提供详细见解。",
        temperature=0.3
    )
    
    return response.text
```

## 客户端要求

LLM 采样需要客户端支持：

- 客户端必须实现采样处理程序来处理请求
- 如果客户端不支持采样，调用 `ctx.sample()` 将失败
- 有关实现客户端采样处理程序的详细信息，请参阅[客户端采样](/zh/clients/sampling)
