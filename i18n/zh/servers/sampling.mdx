---
title: LLM 采样
sidebarTitle: 采样
description: 通过 MCP 上下文请求客户端或配置的提供商进行 LLM 文本生成。
icon: robot
---

import { VersionBadge } from '/snippets/version-badge.mdx'

<VersionBadge version="2.0.0" />

LLM 采样允许 MCP 工具基于提供的消息请求 LLM 文本生成。默认情况下，采样请求发送到客户端的 LLM，但您也可以配置回退处理器或始终使用特定的 LLM 提供商。当工具需要利用 LLM 能力来处理数据、生成响应或执行基于文本的分析时，这非常有用。

## 为什么使用 LLM 采样？

LLM 采样使工具能够：

- **利用 AI 能力**：使用客户端的 LLM 进行文本生成和分析
- **减轻复杂推理负担**：让 LLM 处理需要自然语言理解的任务
- **生成动态内容**：基于数据创建响应、摘要或转换
- **保持上下文**：使用用户已在交互的同一 LLM 实例

### 基本用法

使用 `ctx.sample()` 请求客户端的 LLM 生成文本：

```python {14}
from fastmcp import FastMCP, Context

mcp = FastMCP("SamplingDemo")

@mcp.tool
async def analyze_sentiment(text: str, ctx: Context) -> dict:
    """使用客户端的 LLM 分析文本情感。"""
    prompt = f"""分析以下文本的情感为积极、消极或中性。
    只输出一个词 - '积极'、'消极' 或 '中性'。
    
    要分析的文本：{text}"""
    
    # 请求 LLM 分析
    response = await ctx.sample(prompt)
    
    # 处理 LLM 的响应
    sentiment = response.text.strip().lower()
    
    # 映射到标准情感值
    if "积极" in sentiment or "positive" in sentiment:
        sentiment = "积极"
    elif "消极" in sentiment or "negative" in sentiment:
        sentiment = "消极"
    else:
        sentiment = "中性"
    
    return {"text": text, "sentiment": sentiment}
```

## 方法签名

<Card icon="code" title="上下文采样方法">
<ResponseField name="ctx.sample" type="异步方法">
  从客户端的 LLM 请求文本生成
  
  <Expandable title="参数">
    <ResponseField name="messages" type="str | list[str | SamplingMessage]">
      发送给 LLM 的字符串或字符串/消息对象列表
    </ResponseField>
    
    <ResponseField name="system_prompt" type="str | None" default="None">
      用于指导 LLM 行为的可选系统提示
    </ResponseField>
    
    <ResponseField name="temperature" type="float | None" default="None">
      可选的采样温度（控制随机性，通常为 0.0-1.0）
    </ResponseField>
    
    <ResponseField name="max_tokens" type="int | None" default="512">
      可选的最大生成令牌数
    </ResponseField>
    
    <ResponseField name="model_preferences" type="ModelPreferences | str | list[str] | None" default="None">
      可选的模型选择偏好（例如，模型提示字符串、提示列表或 ModelPreferences 对象）
    </ResponseField>
  </Expandable>
  
  <Expandable title="响应">
    <ResponseField name="response" type="TextContent | ImageContent">
      LLM 的响应内容（通常是带有 .text 属性的 TextContent）
    </ResponseField>
  </Expandable>
</ResponseField>
</Card>

## 简单文本生成

### 基本提示

使用简单字符串提示生成文本：

```python {6}
@mcp.tool
async def generate_summary(content: str, ctx: Context) -> str:
    """生成提供内容的摘要。"""
    prompt = f"请提供以下内容的简洁摘要：\n\n{content}"
    
    response = await ctx.sample(prompt)
    return response.text
```

### 系统提示

使用系统提示来指导 LLM 的行为：

```python {4-9}
@mcp.tool
async def generate_code_example(concept: str, ctx: Context) -> str:
    """为给定概念生成 Python 代码示例。"""
    response = await ctx.sample(
        messages=f"编写一个演示 '{concept}' 的简单 Python 代码示例。",
        system_prompt="你是一个专业的 Python 程序员。提供简洁、可运行的代码示例，不需要解释。",
        temperature=0.7,
        max_tokens=300
    )
    
    code_example = response.text
    return f"```python\n{code_example}\n```"
```


### 模型偏好

为不同用例指定模型偏好：

```python {4-8, 17-22}
@mcp.tool
async def creative_writing(topic: str, ctx: Context) -> str:
    """使用特定模型生成创意内容。"""
    response = await ctx.sample(
        messages=f"写一个关于 {topic} 的创意短篇小说",
        model_preferences="claude-3-sonnet",  # 偏好特定模型
        include_context="thisServer",  # 使用服务器的上下文
        temperature=0.9,  # 高创造性
        max_tokens=1000
    )
    
    return response.text

@mcp.tool
async def technical_analysis(data: str, ctx: Context) -> str:
    """使用专注推理的模型执行技术分析。"""
    response = await ctx.sample(
        messages=f"分析这些技术数据并提供见解：{data}",
        model_preferences=["claude-3-opus", "gpt-4"],  # 偏好推理模型
        temperature=0.2,  # 低随机性保证一致性
        max_tokens=800
    )
    
    return response.text
```

### 复杂消息结构

使用结构化消息进行更复杂的交互：

```python {1, 6-10}
from fastmcp.client.sampling import SamplingMessage

@mcp.tool
async def multi_turn_analysis(user_query: str, context_data: str, ctx: Context) -> str:
    """使用多轮对话结构执行分析。"""
    messages = [
        SamplingMessage(role="user", content=f"我有这些数据：{context_data}"),
        SamplingMessage(role="assistant", content="我能看到您的数据。您希望我分析什么？"),
        SamplingMessage(role="user", content=user_query)
    ]
    
    response = await ctx.sample(
        messages=messages,
        system_prompt="你是一个数据分析师。基于对话上下文提供详细见解。",
        temperature=0.3
    )
    
    return response.text
```

## 采样回退处理器

客户端对采样的支持是可选的。如果客户端不支持采样，服务器将报告错误，表明客户端不支持采样。

但是，您可以为 FastMCP 服务器提供一个 `sampling_handler`，它将采样请求直接发送到 LLM 提供商，而不是通过客户端路由。`sampling_handler_behavior` 参数控制何时使用此处理器：

- **`"fallback"`**（默认）：仅在客户端不支持采样时使用处理器。请求首先发送到客户端，必要时回退到处理器。
- **`"always"`**：始终使用处理器，完全绕过客户端。当您想要完全控制用于采样的 LLM 时很有用。

采样处理器可以使用任何 LLM 提供商实现，但作为贡献模块提供了 OpenAI 的示例实现。采样缺乏典型 LLM 完成的全部能力。因此，指向第三方提供商的 OpenAI 兼容 API 的 OpenAI 采样处理器通常足以实现采样处理器。

### 回退模式（默认）

仅在客户端不支持采样时使用处理器：

```python
import asyncio
import os

from mcp.types import ContentBlock
from openai import OpenAI

from fastmcp import FastMCP
from fastmcp.experimental.sampling.handlers.openai import OpenAISamplingHandler
from fastmcp.server.context import Context


async def async_main():
    server = FastMCP(
        name="OpenAI 采样回退示例",
        sampling_handler=OpenAISamplingHandler(
            default_model="gpt-4o-mini",
            client=OpenAI(
                api_key=os.getenv("API_KEY"),
                base_url=os.getenv("BASE_URL"),
            ),
        ),
        sampling_handler_behavior="fallback",  # 默认 - 仅在客户端不支持采样时使用
    )

    @server.tool
    async def test_sample_fallback(ctx: Context) -> ContentBlock:
        # 如果可用，将使用客户端的 LLM，否则回退到处理器
        return await ctx.sample(
            messages=["hello world!"],
        )

    await server.run_http_async()


if __name__ == "__main__":
    asyncio.run(async_main())
```

### 始终模式

始终使用处理器，绕过客户端：

```python
server = FastMCP(
    name="服务器控制采样",
    sampling_handler=OpenAISamplingHandler(
        default_model="gpt-4o-mini",
        client=OpenAI(api_key=os.getenv("API_KEY")),
    ),
    sampling_handler_behavior="always",  # 始终使用处理器，从不使用客户端
)

@server.tool
async def analyze_data(data: str, ctx: Context) -> str:
    # 将始终使用服务器配置的 LLM，而不是客户端的
    result = await ctx.sample(
        messages=f"分析此数据：{data}",
        system_prompt="您是一个数据分析师。",
    )
    return result.text
```

## 客户端要求

默认情况下，LLM 采样需要客户端支持：

- 客户端必须实现采样处理程序来处理请求（请参阅[客户端采样](/zh/clients/sampling)）
- 如果客户端不支持采样且未配置回退处理器，`ctx.sample()` 将抛出错误
- 配置具有 `sampling_handler_behavior="fallback"` 的 `sampling_handler` 以自动处理不支持采样的客户端
- 使用 `sampling_handler_behavior="always"` 完全绕过客户端并控制使用哪个 LLM
